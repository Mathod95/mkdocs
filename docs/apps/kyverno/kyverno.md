```ad-missing
Modifier les exemples de ce que peut faire Kyverno
```
## Introduction

[Kyverno](https://kyverno.io/) est un outil dÃ©diÃ© Ã  la sÃ©curitÃ© et Ã  la conformitÃ© des clusters Kubernetes. Il permet de crÃ©er et d'appliquer des _policies_ Ã©crites en YAML de maniÃ¨re dÃ©clarative, proposant une approche plus flexible, native et facilite la sÃ©curisation et le contrÃ´le des ressources Kubernetes.

Par exemple, les policies peuvent servir Ã :
- Forcer la mise en place de labels afin dâ€™harmoniser la gestion de vos ressources
- Restreindre la surface dâ€™attaque en interdisant de monter desÂ `hostPath`
- EmpÃªcher un Pod dâ€™utiliser desÂ `hostPort`
- Mettre en place des valeurs par dÃ©faut pour les limitations de ressource

??? note "TLDR"
    Kyverno est un policy engineÂ qui permet:  
    - DÃ©finir des policies en tant que ressources Kubernetes ;  
    - Valider, modifier ou gÃ©nÃ©rer des ressources Ã  la volÃ©e via ces policies ; 
    - Bloquer des ressources non conformes grÃ¢ce Ã  un admission controller ;  
    - Journaliser les violations de policies dans des rapports. 

    Avantages:  
    - **DÃ©finir des policies de sÃ©curitÃ©**Â pour interdire la crÃ©ation de ressources non sÃ©curisÃ©es ;  
    - **Simplifier la vie des Ops**Â via des mutations de ressources Ã  la volÃ©e ;  
    - **PossibilitÃ© de configurer les policies en mode audit**Â (sans blocage) ou enforce ;  
    - Une Ã©criture de policy simple (par rapport Ã  GateKeeper notamment)  

    InconvÃ©nients:  
    - Difficile de crÃ©er des policies avec une logique trÃ¨s spÃ©cifique et/ou complexe ; 
    - Kyverno est un Single Point Of Failure. Certains connaissentÂ [le cÃ´tÃ© sombre des adminssion controller](https://techblog.cisco  .com/blog/dark-side-of-kubernetes-admission-webhooks/)Â : si les pods Kyverno ne sont plus disponibles, plus aucune ressource Kubernetes ne peut se dÃ©ployer sur le cluster. Je vous donnerai quelques astuces pour Ã©viter tout problÃ¨me dans la suite de lâ€™article.


    2 types de policies 
    - **Policy**: S'applique Ã  un namespace spÃ©cifique. Utile pour des configurations particuliÃ¨res Ã  un environnement ou une applic  ation.
    - **ClusterPolicy**: S'applique Ã  l'ensemble du cluster. UtilisÃ©e pour des rÃ¨gles de sÃ©curitÃ© et de conformitÃ© globales.  

    3 types d'actions possible  
    - **Validation**: Kyverno permet de dÃ©finir des rÃ¨gles pour vÃ©rifier que les configurations respectent certains standards avant   de crÃ©er ou modifier un objet. Par exemple, on peut interdire des privilÃ¨ges trop Ã©levÃ©s sur les Pods.
    - **Mutation**: Kyverno peut modifier des configurations par dÃ©faut. Par exemple, si un Pod nâ€™a pas de valeur dÃ©finie pour *labe  l*, Kyverno peut la gÃ©nÃ©rer automatiquement.
    - **GÃ©nÃ©ration**: Il est possible de crÃ©er automatiquement des ressources selon certains dÃ©clencheurs, comme la crÃ©ation dâ€™un _N  amespace_, pour garantir que chaque espace de noms ait une configuration de sÃ©curitÃ© prÃ©configurÃ©e.

    2 modes d'utilisation 
    - **Audit**: Kyverno surveille les violations des _policies_ sans bloquer les opÃ©rations, permettant aux Ã©quipes de voir les inc  idents de non-conformitÃ© sans interrompre les dÃ©ploiements.
    - **Strict**: Kyverno peut empÃªcher la crÃ©ation ou la mise Ã  jour de ressources non conformes, offrant une approche de sÃ©curitÃ©   proactive.


Il existe deux types de _policies_ dans Kyverno `Policy` et `ClusterPolicy`. Voici les diffÃ©rences entre les deux :
- **Policy**: Une _policy_ est une politique de validation, de mutation ou de gÃ©nÃ©ration qui s'applique Ã  un _namespace_ spÃ©cifique dans un cluster Kubernetes. Les _policies_ sont utiles pour gÃ©rer des rÃ¨gles qui ne concernent qu'un environnement ou une application particuliÃ¨re, offrant ainsi une granularitÃ© dans l'application des rÃ¨gles de sÃ©curitÃ© et de conformitÃ©. Par exemple, si vous avez plusieurs namespaces pour diffÃ©rents environnements (dÃ©veloppement, test, production), vous pouvez crÃ©er des _policies_ spÃ©cifiques Ã  chacun de ces namespaces.
- **ClusterPolicy**: Une **ClusterPolicy**, en revanche, est une politique qui s'applique Ã  l'ensemble du cluster Kubernetes, sans restriction de namespace. Les _ClusterPolicies_ sont utilisÃ©es pour dÃ©finir des rÃ¨gles de sÃ©curitÃ© ou de conformitÃ© qui doivent Ãªtre appliquÃ©es de maniÃ¨re globale Ã  toutes les ressources du cluster. Cela peut inclure des exigences de sÃ©curitÃ© fondamentales, comme l'utilisation de configurations spÃ©cifiques pour tous les Pods dans le cluster, indÃ©pendamment du namespace.

!!! info
    Ces deux ressources gÃ©nÃ¨rent desÂ `PolicyReports`Â (ou desÂ `ClusterPolicyReports`)

C'est policy peuvent avoir 3 type d'actions !
- **Validation** : Kyverno permet de dÃ©finir des rÃ¨gles pour vÃ©rifier que les configurations respectent certains standards avant de crÃ©er ou modifier un objet. Par exemple, on peut interdire des privilÃ¨ges trop Ã©levÃ©s sur les Pods.
- **Mutation** : Kyverno peut modifier des configurations par dÃ©faut. Par exemple, si un Pod nâ€™a pas de valeur dÃ©finie pour *label*, Kyverno peut la gÃ©nÃ©rer automatiquement.
- **GÃ©nÃ©ration** : Il est possible de crÃ©er automatiquement des ressources selon certains dÃ©clencheurs, comme la crÃ©ation dâ€™un _Namespace_, pour garantir que chaque espace de noms ait une configuration de sÃ©curitÃ© prÃ©configurÃ©e.

Il existe 2 type de mode **Audit** et mode dâ€™application **Strict** :
- En mode **audit**, Kyverno surveille les violations des _policies_ sans bloquer les opÃ©rations, permettant aux Ã©quipes de voir les incidents de non-conformitÃ© sans interrompre les dÃ©ploiements.
- En mode **strict**, Kyverno peut empÃªcher la crÃ©ation ou la mise Ã  jour de ressources non conformes, offrant une approche de sÃ©curitÃ© proactive.

Kyverno passe Ã  lâ€™action Ã  diffÃ©rents moments clÃ©s du cycle de vie des objets Kubernetes, en fonction du type de _policy_ et de lâ€™action dÃ©finie. Voici les principales situations oÃ¹ Kyverno intervient :
- Ã€ la crÃ©ation ou modification dâ€™un objet Kubernetes
- Lors dâ€™un Ã©vÃ©nement dÃ©clencheur pour les policies de gÃ©nÃ©ration
- Pendant les audits pÃ©riodiques

Kyverno intervient au moyen dâ€™un Admission Controller dynamique qui traite des callbacks (webhooks dâ€™admissions) de validation et de mutation en provenance de lâ€™apiserver Kubernetes.

LesÂ `Policies`Â Kyverno sont dÃ©coupÃ©es en 3 parties:
- Les rÃ¨gles, ayant chacune un sÃ©lecteur et une action
- Le sÃ©lecteur de ressource de la rÃ¨gle, en inclusion ou en exclusion,
- Lâ€™action de la rÃ¨gle, qui peut Ãªtre une validation, une mutation ou une gÃ©nÃ©ration de ressource
## GÃ©rer ses Policies sur Kubernetes avec Kyverno

DÃ©jÃ  mentionnÃ© dansÂ [notre article](https://particule.io/blog/kubernetes-psp-deprecated/)Â qui annonÃ§ait la dÃ©prÃ©ciation des PodSecurityPolicies et leur suppression Ã  partir de la version 1.25,Â [Kyverno](https://kyverno.io/)Â sâ€™annonce Ãªtre un digne successeur Ã  ces derniÃ¨res.

Pour rappel, les PSP permettaient aux administrateurs de cluster de forcer lâ€™application de bonnes pratiques en dÃ©finissant un ensemble de rÃ¨gles qui Ã©taient exÃ©cutÃ©es lors de la phase dâ€™admission, au moyen dâ€™un Admission ControllerÂ _compiled-in_.

Dans cet article, nous verrons comment mettre en place Kyverno, dÃ©couvrir ses fonctionnalitÃ©s et explorer ce que nous offre cette mÃ©thode de gestion des Policies.
### Objectifs
- 
### PrÃ©requis
- Docker desktop installer
### Ma configuration
- Debian 12
- Kind v0.24.0
- [Docker desktop](https://desktop.docker.com/win/main/amd64/Docker%20Desktop%20Installer.exe?utm_source=docker&utm_medium=webreferral&utm_campaign=dd-smartbutton&utm_location=module)Â - Â 4.34.3

---
## Mise en place de Kyverno
## Installation

CommenÃ§ons par mettre en place un environnement pour explorer les possibilitÃ©s de Kyverno! En utilisantÂ **kind**Â (prÃ©sentÃ© surÂ [notre blog](https://particule.io/blog/kubernetes-local/)Â !), configurons un cluster local:

``` bash hl_lines="1-3"
[ $(uname -m) = x86_64 ] && curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.24.0/kind-linux-amd64
chmod +x ./kind
sudo mv ./kind /usr/local/bin/kind
```

``` bash hl_lines="1"
kind version
kind v0.24.0 go1.22.6 linux/amd64
```

``` bash hl_lines="1"
kind create cluster --name kyverno
Creating cluster "kyverno" ...
 âœ“ Ensuring node image (kindest/node:v1.31.0) ğŸ–¼
 âœ“ Preparing nodes ğŸ“¦
 âœ“ Writing configuration ğŸ“œ
 âœ“ Starting control-plane ğŸ•¹ï¸
 âœ“ Installing CNI ğŸ”Œ
 âœ“ Installing StorageClass ğŸ’¾
Set kubectl context to "kind-kyverno"
You can now use your cluster with:

kubectl cluster-info --context kind-kyverno

Thanks for using kind! ğŸ˜Š
```

Il existe diffÃ©rentes mÃ©thodes dâ€™installation pour mettre en place Kyverno, utilisons le Chart Helm officiel pour bootstrapper notre cluster.

``` bash hl_lines="1 4 9"
helm repo add kyverno https://kyverno.github.io/kyverno/
"kyverno" has been added to your repositories

helm repo update
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "kyverno" chart repository
Update Complete. âˆHappy Helming!âˆ

helm install kyverno kyverno/kyverno --namespace kyverno --create-namespace
NAME: kyverno
LAST DEPLOYED: Sat Oct 19 19:41:37 2024
NAMESPACE: kyverno
STATUS: deployed
REVISION: 1
NOTES:
Chart version: 3.2.7
Kyverno version: v1.12.6

Thank you for installing kyverno! Your release is named kyverno.

The following components have been installed in your cluster:
- CRDs
- Admission controller
- Reports controller
- Cleanup controller
- Background controller


âš ï¸  WARNING: Setting the admission controller replica count below 2 means Kyverno is not running in high availability mode.

ğŸ’¡ Note: There is a trade-off when deciding which approach to take regarding Namespace exclusions. Please see the documentation at https://kyverno.io/docs/installation/#security-vs-operability to understand the risks.
```

!!! warning
    Vous remarquerez le **WARNING** lors de l'installation. Par dÃ©faut, Kyverno est dÃ©ployÃ© avec un seul rÃ©plica. En cas de crash, il devient impossible de dÃ©ployer ou de modifier quoi que ce soit sur le cluster. La solution consiste Ã  supprimer les **webhooks** de Kyverno pour dÃ©bloquer la situation. Il est Ã©galement recommandÃ© d'augmenter ces limit en ressources (principalement RAM) et dâ€™opter pour 3 rÃ©plicas (ou plus) afin de garantir plus de stabilitÃ©.


!!! note
    Il est probablement prÃ©fÃ©rable dâ€™exclure certains namespaces (kube-system, kyverno, â€¦) du scope de Kyverno pour Ã©viter de mauvaises surprises. Le blogpostÂ _Thibault Lengagne_Â de chez Padok en parle aussi (lien en bas de lâ€™article).


``` bash hl_lines="1"
kubectl -n kyverno get all
NAME                                                           READY   STATUS      RESTARTS   AGE
pod/kyverno-admission-controller-84bc98464d-ppb8d              1/1     Running     0          57m
pod/kyverno-background-controller-86cdcb7fb6-prqst             1/1     Running     0          57m
pod/kyverno-cleanup-admission-reports-28822710-np9hw           0/1     Completed   0          8m57s
pod/kyverno-cleanup-cluster-admission-reports-28822710-fmj4g   0/1     Completed   0          8m57s
pod/kyverno-cleanup-cluster-ephemeral-reports-28822710-qphr5   0/1     Completed   0          8m57s
pod/kyverno-cleanup-controller-79cb7b5d87-qgl2l                1/1     Running     0          57m
pod/kyverno-cleanup-ephemeral-reports-28822710-cw5f7           0/1     Completed   0          8m57s
pod/kyverno-reports-controller-7c845bdf65-dkv4f                1/1     Running     0          57m

NAME                                            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
service/kyverno-background-controller-metrics   ClusterIP   10.96.93.154    <none>        8000/TCP   57m
service/kyverno-cleanup-controller              ClusterIP   10.96.91.249    <none>        443/TCP    57m
service/kyverno-cleanup-controller-metrics      ClusterIP   10.96.182.4     <none>        8000/TCP   57m
service/kyverno-reports-controller-metrics      ClusterIP   10.96.75.92     <none>        8000/TCP   57m
service/kyverno-svc                             ClusterIP   10.96.60.167    <none>        443/TCP    57m
service/kyverno-svc-metrics                     ClusterIP   10.96.251.154   <none>        8000/TCP   57m

NAME                                            READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/kyverno-admission-controller    1/1     1            1           57m
deployment.apps/kyverno-background-controller   1/1     1            1           57m
deployment.apps/kyverno-cleanup-controller      1/1     1            1           57m
deployment.apps/kyverno-reports-controller      1/1     1            1           57m

NAME                                                       DESIRED   CURRENT   READY   AGE
replicaset.apps/kyverno-admission-controller-84bc98464d    1         1         1       57m
replicaset.apps/kyverno-background-controller-86cdcb7fb6   1         1         1       57m
replicaset.apps/kyverno-cleanup-controller-79cb7b5d87      1         1         1       57m
replicaset.apps/kyverno-reports-controller-7c845bdf65      1         1         1       57m

NAME                                                      SCHEDULE       TIMEZONE   SUSPEND   ACTIVE   LAST SCHEDULE   AGE
cronjob.batch/kyverno-cleanup-admission-reports           */10 * * * *   <none>     False     0        8m57s           57m
cronjob.batch/kyverno-cleanup-cluster-admission-reports   */10 * * * *   <none>     False     0        8m57s           57m
cronjob.batch/kyverno-cleanup-cluster-ephemeral-reports   */10 * * * *   <none>     False     0        8m57s           57m
cronjob.batch/kyverno-cleanup-ephemeral-reports           */10 * * * *   <none>     False     0        8m57s           57m

NAME                                                           STATUS     COMPLETIONS   DURATION   AGE
job.batch/kyverno-cleanup-admission-reports-28822710           Complete   1/1           3s         8m57s
job.batch/kyverno-cleanup-cluster-admission-reports-28822710   Complete   1/1           3s         8m57s
job.batch/kyverno-cleanup-cluster-ephemeral-reports-28822710   Complete   1/1           3s         8m57s
job.batch/kyverno-cleanup-ephemeral-reports-28822710           Complete   1/1           3s         8m57s
```

Nous pouvons lister les CRDs installÃ©s par le chart Helm:

``` bash hl_lines="1 15"
kubectl api-resources --api-group kyverno.io
NAME                           SHORTNAMES   APIVERSION            NAMESPACED   KIND
admissionreports               admr         kyverno.io/v2         true         AdmissionReport
backgroundscanreports          bgscanr      kyverno.io/v2         true         BackgroundScanReport
cleanuppolicies                cleanpol     kyverno.io/v2         true         CleanupPolicy
clusteradmissionreports        cadmr        kyverno.io/v2         false        ClusterAdmissionReport
clusterbackgroundscanreports   cbgscanr     kyverno.io/v2         false        ClusterBackgroundScanReport
clustercleanuppolicies         ccleanpol    kyverno.io/v2         false        ClusterCleanupPolicy
clusterpolicies                cpol         kyverno.io/v1         false        ClusterPolicy
globalcontextentries           gctxentry    kyverno.io/v2alpha1   false        GlobalContextEntry
policies                       pol          kyverno.io/v1         true         Policy
policyexceptions               polex        kyverno.io/v2         true         PolicyException
updaterequests                 ur           kyverno.io/v2         true         UpdateRequest

kubectl api-resources --api-group wgpolicyk8s.io
NAME                   SHORTNAMES   APIVERSION                NAMESPACED   KIND
clusterpolicyreports   cpolr        wgpolicyk8s.io/v1alpha2   false        ClusterPolicyReport
policyreports          polr         wgpolicyk8s.io/v1alpha2   true         PolicyReport
```

Les rÃ¨gles du profile â€œdefaultâ€ sont dÃ©finies via desÂ `ClusterPolicies`:

``` bash hl_lines="1"
$ kubectl get cpol -n kyverno
NAME                             BACKGROUND   ACTION
disallow-add-capabilities        true         audit
disallow-host-namespaces         true         audit
disallow-host-path               true         audit
disallow-host-ports              true         audit
disallow-privileged-containers   true         audit
disallow-selinux                 true         audit
require-default-proc-mount       true         audit
restrict-apparmor-profiles       true         audit
restrict-sysctls                 true         audit
```

En effet, nous avons dÃ©jÃ  un bon nombre deÂ `Policies`Â ! Cependant, pas dâ€™inquiÃ©tude. Nous pouvons observer quâ€™elles ont toutes Ã©tÃ© dÃ©finies avecÂ `background=true`Â etÂ `action=audit`. Cette configuration indique Ã  Kyverno que ces rÃ¨gles ne doivent pas Ãªtre bloquantes. Toutes les 15 minutes, une analyse est lancÃ©e pour chaque rÃ¨gle sur les ressources prÃ©sentes Ã  cet instant au sein du Cluster et gÃ©nÃ¨rent des ClusterPolicyReports.

### Exemples dâ€™utilisation

#### CrÃ©ation dâ€™une ClusterPolicy

Il est temps de mettre en place notre premiÃ¨reÂ `Policy`Â ! DÃ©finissons une rÃ¨gle simple qui vÃ©rifie la prÃ©sence dâ€™un label identifiant le nom de lâ€™application Ã  laquelle unÂ `Pod`Â correspond.

``` yaml linenums="1"
---
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: require-labels
spec:
  validationFailureAction: enforce
  rules:
    - name: check-for-label-name
      match:
        resources:
          kinds:
            - Pod
      validate:
        message: "label `app.kubernetes.io/name` is required"
        pattern:
          metadata:
            labels:
              app.kubernetes.io/name: "?*"
```

La rÃ¨gleÂ `check-for-label-name`Â permet de considÃ©rer comme prÃ©requis le labelÂ `app.kubernetes.io/name`Â pour le dÃ©ploiement des ressources de typeÂ `Pod`.

Pour rÃ©sumer cette rÃ¨gle:

_Il est obligatoire pour toutÂ `Pod`Â dâ€™avoir un labelÂ `app.kubernetes.io/name`Â dâ€™une longueur supÃ©rieure ou Ã©gale Ã  1 caractÃ¨re._

AprÃ¨s avoir appliquÃ© notreÂ `ClusterPolicy`, nous obtenons la ressource suivante:

``` yaml linenums="1"
---
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: require-labels
  annotations:
    pod-policies.kyverno.io/autogen-controllers: DaemonSet,Deployment,Job,StatefulSet,CronJob
spec:
  background: true
  rules:
    - match:
      resources:
        kinds:
          - Pod
      name: check-for-label-name
      validate:
        message: label `app.kubernetes.io/name` is required
      pattern:
        metadata:
          labels:
            app.kubernetes.io/name: ?*
    - match:
      resources:
        kinds:
          - DaemonSet
          - Deployment
          - Job
          - StatefulSet
      name: autogen-check-for-label-name
      validate:
        message: label `app.kubernetes.io/name` is required
        pattern:
          spec:
            template:
              metadata:
                labels:
                  app.kubernetes.io/name: ?*
    - match:
      resources:
        kinds:
          - CronJob
      name: autogen-cronjob-check-for-label-name
      validate:
        message: label `app.kubernetes.io/name` is required
        pattern:
          spec:
            jobTemplate:
              spec:
                template:
                  metadata:
                    labels:
                      app.kubernetes.io/name: ?*
      validationFailureAction: enforce
```

On retrouve notre rÃ¨gle â€œcheck-for-label-nameâ€, mais deux autres rÃ¨gles ont Ã©tÃ© autogÃ©nÃ©rÃ©es afin dâ€™appliquer le comportement que nous souhaitons aux autres ressources permettant de dÃ©finir des pods.

Ces rÃ¨gles sont crÃ©Ã©es par lesÂ `autogen-controllers`Â ([link](https://kyverno.io/docs/writing-policies/autogen/)) rÃ©fÃ©rencÃ©s par lâ€™annotation correspondante qui sert dâ€™annotation par dÃ©faut. DÃ©finir manuellement cette annotation permet de modifier le comportement de gÃ©nÃ©ration automatique de rÃ¨gles.

Essayons de dÃ©ployer une ressource qui enfreint la rÃ¨gle que nous venons de crÃ©er:

``` bash hl_lines="1 8"
$ kubectl run sample --image=busybox
Error from server: admission webhook "validate.kyverno.svc" denied the request:

resource Pod/default/sample was blocked due to the following policies

require-labels:
  check-for-labels: 'validation error: label `app.kubernetes.io/name` is required. Rule check-for-label-name failed at path /metadata/labels/app.kubernetes.io/name/'
$ kubectl run sample --image=busybox  --labels app.kubernetes.io/name=valid
pod/sample created
```

NotreÂ `Policy`Â a uneÂ `validationFailureAction=enforce`, il nous est donc impossible de crÃ©er une ressource ne validant pas lâ€™intÃ©gralitÃ© des rÃ¨gles dÃ©finies!

AprÃ¨s avoir ajoutÃ© un label pour le nom de notreÂ `Pod`, le webhook dâ€™admission Kyverno valide la requÃªte et la ressource est crÃ©Ã©e.

#### Le mode â€œauditâ€

Lors du dÃ©ploiement du Chart Helm, un profil par dÃ©faut a Ã©tÃ© configurÃ© en mode audit.

_We have installed the â€œdefaultâ€ profile of Pod Security Standards and set them in audit mode._

Ce mode de validation deÂ `Policy`Â estÂ _non-intrusif_Â et permet de mettre progressivement en application les rÃ¨gles. Il nâ€™empÃªche pas la crÃ©ation des ressources qui ne sont pas conformes Ã  laÂ `Policy`Â mais va historiser les infractions dans desÂ `PolicyReport`Â (`polr`) si la ressource estÂ _namespaced_Â (comme unÂ `Deployment`Â ou unÂ `ConfigMap`) dans le namespace de la ressource, ou desÂ `ClusterPolicyReports`Â (`cpolr`) sâ€™il sâ€™agit de ressource nonÂ _namespaced_Â (unÂ `Namespace`)

Lors de la crÃ©ation dâ€™unÂ `Deployment`, le template duÂ `Pod`Â enfreint uneÂ `ClusterPolicy`. Lâ€™infraction sera reportÃ©e dans uneÂ `PolicyReport`Â dans le namespace duÂ `Pod`.

Lors de la crÃ©ation dâ€™uneÂ `IngressClass`, lâ€™absence de label enfreint une autreÂ `ClusterPolicy`. Lâ€™infraction sera reportÃ©e dans uneÂ `ClusterPolicyReport`.

Comme mentionnÃ© prÃ©cÃ©demment, le mode audit va pÃ©riodiquement (toutes les 15 minutes) analyser les ressources et gÃ©nÃ©rer des reports.

Configurons uneÂ `Policy`Â en mode audit ainsi quâ€™unÂ `Pod`Â de test:

``` yaml linenums="1"
---
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: require-tier-on-pod
spec:
  validationFailureAction: audit
  background: true
  rules:
    - name: check-for-tier-on-pod
      match:
        resources:
          kinds:
            - Pod
      validate:
        message: "if set, label `app.kubernetes.io/tier` must be any of ['frontend', 'backend', 'internal']"
        pattern:
          metadata:
            labels:
              =(app.kubernetes.io/tier): "frontend | backend | internal"
---
apiVersion: v1
kind: Pod
metadata:
  labels:
    app.kubernetes.io/tier: uknown
    app.kubernetes.io/name: example-pod
  name: bad-tier
spec:
  containers:
    - image: nginx:latest
      name: example-tier
```

_Si le labelÂ `app.kubernetes.io/tier`Â existe, il doit correspondre Ã  lâ€™une des valeurs frontend, backend ou internal._

AprÃ¨s quelques instants, nous pouvons observer un avertissement venant de lâ€™admission-controller:

``` bash hl_lines="1 8"
$ kubectl describe pod bad-tier  | grep -A 5 Events
Events:
  Type     Reason           Age   From                  Message
  ----     ------           ----  ----                  -------
  Warning  PolicyViolation  8s   admission-controller  Rule(s) 'check-for-tier-on-pod' of policy 'require-tier-on-pod' failed to apply on the resource
  Normal   Scheduled        8s   default-scheduler     Successfully assigned default/bad-tier to kyverno-control-plane
  Normal   Pulling          7s   kubelet               Pulling image "nginx:latest"
$ kubectl get polr
NAME              PASS   FAIL   WARN   ERROR   SKIP   AGE
polr-ns-default   1      1      0      0       0      30m
```

Nous pouvons obtenir unÂ _report_Â complet en effectuant unÂ _describe_Â sur leÂ `PolicyReport`Â duÂ `Namespace`Â correspondant.

``` yaml linenums="1"
---
apiVersion: wgpolicyk8s.io/v1alpha1
kind: PolicyReport
metadata:
  name: polr-ns-default
  namespace: default
results:
  - message: 'validation error: if set, label `app.kubernetes.io/tier` must be any of
    [''frontend'', ''backend'', ''internal'']. Rule check-for-tier-on-pod failed at
    path /metadata/labels/app.kubernetes.io/tier/'
    policy: require-tier-on-pod
    resources:
      - apiVersion: v1
        kind: Pod
        name: bad-tier
        namespace: default
        uid: 6e743322-5b2a-4ad7-bc79-eca437ab82db
    rule: check-for-tier-on-pod
    scored: true
    status: fail
  - category: Pod Security Standards (Default)
    message: validation rule 'host-namespaces' passed.
    policy: disallow-host-namespaces
    resources:
      - apiVersion: v1
        kind: Pod
        name: sample
        namespace: default
        uid: 19dc0c5b-3960-4178-8bb0-6cd61a23c089
    rule: host-namespaces
    scored: true
    status: pass
summary:
  error: 0
  fail: 1
  pass: 1
  skip: 0
  warn: 0
```

LesÂ `PolicyReports`Â permettent dâ€™avoir une vue dâ€™ensemble des infractions locales au namespace dans lequel elles rÃ©sident.

Chaque report inclut un rÃ©capitulatif de lâ€™application des rÃ¨gles avec le nombre dâ€™infraction, de validation, dâ€™avertissements, â€¦

### Comment implÃ©menter Kyverno

#### Lors de la mise en place dâ€™un Cluster

Afin de partir sur de bonnes bases, vous pouvez intÃ©grer Kyverno Ã  votre Cluster Kubernetes aprÃ¨s sa crÃ©ation en surchargeantÂ [les valeurs par dÃ©faut](https://github.com/kyverno/kyverno/blob/main/charts/kyverno/values.yaml)Â du Chart Helm.

Une version plus sÃ©curisÃ©e du profile â€œ[default](https://kyverno.io/policies/pod-security/default/)â€ est disponible via le profile â€œ[restricted](https://kyverno.io/policies/pod-security/restricted/)â€ et peut Ãªtre accompagnÃ©e dâ€™unÂ `validationFailureAction=enforce`Â afin de garantir lâ€™intÃ©gritÃ© et la sÃ©curitÃ© du cluster.

#### Dans un Cluster existant

Il est totalement possible dâ€™ajouter Kyverno Ã  une configuration en utilisant le mÃªme procÃ©dÃ©. Attention cependant Ã  ne pas enforce les Policies que vous mettez en place de faÃ§on Ã  ne pas cause dâ€™interruption.

Une transition vers des ressources conformes aux rÃ¨gles que vous souhaitez dÃ©finir pourrait sâ€™effectuer de en utilisant la mÃ©thode suivante:

- DÃ©finition des rÃ¨gles Ã  implÃ©menter via Kyverno en mode audit
- Observer les infractions via les PolicyReports et les ClusterPolicyReports
- Ajout dâ€™une Ã©tape de validation dans les CI/CD viaÂ [Kyverno CLI](https://kyverno.io/docs/kyverno-cli/)
- Passage en mode â€œenforceâ€ lorsque les ressources sont conformes Ã  la Policy


## Conclusion

La mise en place dâ€™une solution de gestion de Policy est une Ã©tape importante concernant la sÃ©curisation des clusters Kubernetes.

Kyverno rÃ©ussit Ã  rÃ©pondre Ã  cette problÃ©matique dâ€™une maniÃ¨re Ã©lÃ©gante et peut se rÃ©vÃ©ler Ãªtre un excellent remplaÃ§ant aux PSP. La dimensionÂ _Kubernetes Native_Â rend la courbe dâ€™apprentissage trÃ¨s douce, ne nÃ©cessitant pas dâ€™apprendre de nouvelle syntaxe.

Dans cet article, nous avons couvert les similaritÃ©s au niveau des PSP. Cependant, Kyverno propose des fonctionnalitÃ©s additionnelles, telle que laÂ [mutation](https://kyverno.io/docs/writing-policies/mutate/)Â de ressources permettant de rajouter des valeurs par dÃ©fauts, ou encore laÂ [gÃ©nÃ©ration](https://kyverno.io/docs/writing-policies/validate/)Â de ressources qui offre la possibilitÃ© de rÃ©pliquer des ressources Ã  travers diffÃ©rents namespaces ou dâ€™automatiser certaines opÃ©rations.

[**Theo â€œBobâ€ Massard**](https://www.linkedin.com/in/tbobm/), Cloud Native Engineer

### En rapport avec cet article
### Liens utile
